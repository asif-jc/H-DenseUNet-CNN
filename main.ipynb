{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "# # import nibabel as nib\n",
    "import SimpleITK as sitk\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "# os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "# import keras.api._v2.keras as keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, BatchNormalization, Activation, Conv2DTranspose, concatenate, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.spatial.distance import directed_hausdorff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Loading and preprocessing training data...\n",
      "------------------------------\n",
      "R_tibia_15A\n",
      "Height of Paitent in mm:  1523.5\n",
      "Length of Paitent AOI (tibia) in mm:  314.8372802734375\n",
      "AOI Slice Start:  684\n",
      "AOI Slice End:  893\n",
      "AOI Slice Range:  210\n",
      "Mask Slices Normalized to MRI Scans Shape (Purely AOI):  (210, 512, 512)\n",
      "\n",
      "\n",
      "Number of Paitents:  1\n",
      "Training Scans Input Shape:  (209, 512, 512, 1)\n",
      "Training Masks Input Shape:  (209, 512, 512, 1)\n",
      "Memory usage of training_scans: 0 MB.\n",
      "Memory usage of train_mask_tibia_labels: 0 MB.\n",
      "\n",
      "\n",
      "Final Training Image Input Shape:  (100, 512, 512, 1)\n",
      "Final Training Mask Input Shape:  (100, 512, 512, 1)\n",
      "------------------------------\n",
      "Completed Preprocessing Stage!\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import preprocessing\n",
    "\n",
    "print('-'*30)\n",
    "print('Loading and preprocessing training data...')\n",
    "print('-'*30)\n",
    "\n",
    "# \\\\files.auckland.ac.nz\\research\\resmed202100086-tws ----> Address for raw data\n",
    "scans_path = 'D:/MRI - Tairawhiti'\n",
    "# filename_labels = ['R_tibia_15A', 'R_tibia_16A', 'R_tibia_4A']\n",
    "filename_labels = ['R_tibia_15A']\n",
    "imgs_train, imgs_mask_train  = preprocessing(scans_path, filename_labels)\n",
    "\n",
    "#Sample size (temp)\n",
    "imgs_train = imgs_train[:100, :, :, :]\n",
    "imgs_mask_train = imgs_mask_train[:100, :, :, :]\n",
    "\n",
    "# Normalization\n",
    "imgs_mask_train = imgs_mask_train.astype('float32')\n",
    "imgs_train = imgs_train.astype('float32')\n",
    "imgs_mask_train /= 255.  # scale masks to [0, 1]\n",
    "imgs_train /= 255.  # scale masks to [0, 1]\n",
    "print(\"\\n\")\n",
    "print('Final Training Image Input Shape: ', imgs_train.shape)\n",
    "print('Final Training Mask Input Shape: ', imgs_mask_train.shape)\n",
    "\n",
    "print('-'*30)\n",
    "print('Completed Preprocessing Stage!')\n",
    "print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 512, 512, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet(scale = 0.5, dropout_rate = 0.4):\n",
    "    inputs = keras.Input((512,512,1))\n",
    "\n",
    "    # Encoding Path of the UNet (32-64-128-256-512)\n",
    "    conv1   = Conv2D(32*scale, (3, 3), padding=\"same\", activation='relu')(inputs)\n",
    "    # conv1 = Conv2D(32*scale, 3, activation='relu', padding='same')(inputs)\n",
    "    drop1   = Dropout(rate=dropout_rate)(conv1, training=True)\n",
    "    max1    = MaxPooling2D((2, 2))(drop1)\n",
    "\n",
    "    conv2   = Conv2D(64*scale, (3, 3), padding=\"same\", activation='relu')(max1)\n",
    "    drop2   = Dropout(rate=dropout_rate)(conv2, training=True)\n",
    "    max2    = MaxPooling2D((2, 2))(drop2)\n",
    "\n",
    "    conv3   = Conv2D(128*scale, (3, 3), padding=\"same\", activation='relu')(max2)\n",
    "    drop3   = Dropout(rate=dropout_rate)(conv3, training=True)\n",
    "    max3    = MaxPooling2D((2, 2))(drop3)\n",
    "\n",
    "    conv4   = Conv2D(256*scale, (3, 3), padding=\"same\", activation='relu')(max3)\n",
    "    drop4   = Dropout(rate=dropout_rate)(conv4, training=True)\n",
    "    max4    = MaxPooling2D((2, 2))(drop4)\n",
    "\n",
    "    lat     = Conv2D(512*scale, (3, 3), padding=\"same\", activation='relu')(max4)\n",
    "    drop5   = Dropout(rate=dropout_rate)(lat, training=True)\n",
    "\n",
    "    # Decoding Path of the UNet\n",
    "    up1     = UpSampling2D((2, 2))(drop5)\n",
    "    concat1 = concatenate([conv4, up1], axis=-1)\n",
    "    conv5   = Conv2D(256*scale, (3, 3), padding=\"same\", activation='relu')(concat1)\n",
    "    drop6   = Dropout(rate=dropout_rate)(conv5, training=True)\n",
    "    \n",
    "    up2     = UpSampling2D((2, 2))(drop6)\n",
    "    concat2 = concatenate([conv3, up2], axis=-1)\n",
    "    conv6   = Conv2D(128*scale, (3, 3), padding=\"same\", activation='relu')(concat2)\n",
    "    drop7   = Dropout(rate=dropout_rate)(conv6, training=True)\n",
    "    \n",
    "    up3     = UpSampling2D((2, 2))(drop7)\n",
    "    concat3 = concatenate([conv2, up3], axis=-1)\n",
    "    conv7   = Conv2D(64*scale, (3, 3), padding=\"same\", activation='relu')(concat3)\n",
    "    drop8   = Dropout(rate=dropout_rate)(conv7, training=True)\n",
    "\n",
    "    up4     = UpSampling2D((2, 2))(drop8)\n",
    "    concat4 = concatenate([conv1, up4], axis=-1)\n",
    "    conv8   = Conv2D(32*scale, (3, 3), padding=\"same\", activation='relu')(concat4)\n",
    "    drop9   = Dropout(rate=dropout_rate)(conv8, training=True)\n",
    "    \n",
    "    outputs = Conv2D(1, (1, 1), activation=\"softmax\")(drop9)\n",
    "\n",
    "    model   = Model(inputs, outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Loading and preprocessing train data...\n",
      "------------------------------\n",
      "Training Image Input Shape:  (80, 512, 512, 1)\n",
      "Training Mask Input Shape:  (80, 512, 512, 1)\n",
      "Validation Image Input Shape:  (20, 512, 512, 1)\n",
      "Validation Mask Input Shape:  (20, 512, 512, 1)\n",
      "------------------------------\n",
      "Creating and compiling model...\n",
      "------------------------------\n",
      "Epoch 1/200\n",
      "3/3 [==============================] - 126s 39s/step - loss: 0.6529 - val_loss: 0.4214\n",
      "Epoch 2/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GGPC\\anaconda3\\envs\\Py39-CNN\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 127s 40s/step - loss: 0.2659 - val_loss: 0.0502\n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 126s 40s/step - loss: 0.0429 - val_loss: 0.0243\n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 128s 41s/step - loss: 0.0183 - val_loss: 0.0108\n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 132s 42s/step - loss: 0.0078 - val_loss: 0.0098\n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 138s 42s/step - loss: 0.0072 - val_loss: 0.0106\n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 130s 41s/step - loss: 0.0078 - val_loss: 0.0109\n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 130s 41s/step - loss: 0.0078 - val_loss: 0.0100\n",
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)       [(None, 512, 512, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_140 (Conv2D)         (None, 512, 512, 16)         160       ['input_15[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_126 (Dropout)       (None, 512, 512, 16)         0         ['conv2d_140[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_56 (MaxPooli  (None, 256, 256, 16)         0         ['dropout_126[0][0]']         \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_141 (Conv2D)         (None, 256, 256, 32)         4640      ['max_pooling2d_56[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_127 (Dropout)       (None, 256, 256, 32)         0         ['conv2d_141[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_57 (MaxPooli  (None, 128, 128, 32)         0         ['dropout_127[0][0]']         \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_142 (Conv2D)         (None, 128, 128, 64)         18496     ['max_pooling2d_57[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_128 (Dropout)       (None, 128, 128, 64)         0         ['conv2d_142[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_58 (MaxPooli  (None, 64, 64, 64)           0         ['dropout_128[0][0]']         \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_143 (Conv2D)         (None, 64, 64, 128)          73856     ['max_pooling2d_58[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_129 (Dropout)       (None, 64, 64, 128)          0         ['conv2d_143[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_59 (MaxPooli  (None, 32, 32, 128)          0         ['dropout_129[0][0]']         \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_144 (Conv2D)         (None, 32, 32, 256)          295168    ['max_pooling2d_59[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_130 (Dropout)       (None, 32, 32, 256)          0         ['conv2d_144[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_56 (UpSampli  (None, 64, 64, 256)          0         ['dropout_130[0][0]']         \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate_56 (Concatenat  (None, 64, 64, 384)          0         ['conv2d_143[0][0]',          \n",
      " e)                                                                  'up_sampling2d_56[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_145 (Conv2D)         (None, 64, 64, 128)          442496    ['concatenate_56[0][0]']      \n",
      "                                                                                                  \n",
      " dropout_131 (Dropout)       (None, 64, 64, 128)          0         ['conv2d_145[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_57 (UpSampli  (None, 128, 128, 128)        0         ['dropout_131[0][0]']         \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate_57 (Concatenat  (None, 128, 128, 192)        0         ['conv2d_142[0][0]',          \n",
      " e)                                                                  'up_sampling2d_57[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_146 (Conv2D)         (None, 128, 128, 64)         110656    ['concatenate_57[0][0]']      \n",
      "                                                                                                  \n",
      " dropout_132 (Dropout)       (None, 128, 128, 64)         0         ['conv2d_146[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_58 (UpSampli  (None, 256, 256, 64)         0         ['dropout_132[0][0]']         \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate_58 (Concatenat  (None, 256, 256, 96)         0         ['conv2d_141[0][0]',          \n",
      " e)                                                                  'up_sampling2d_58[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_147 (Conv2D)         (None, 256, 256, 32)         27680     ['concatenate_58[0][0]']      \n",
      "                                                                                                  \n",
      " dropout_133 (Dropout)       (None, 256, 256, 32)         0         ['conv2d_147[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_59 (UpSampli  (None, 512, 512, 32)         0         ['dropout_133[0][0]']         \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate_59 (Concatenat  (None, 512, 512, 48)         0         ['conv2d_140[0][0]',          \n",
      " e)                                                                  'up_sampling2d_59[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_148 (Conv2D)         (None, 512, 512, 16)         6928      ['concatenate_59[0][0]']      \n",
      "                                                                                                  \n",
      " dropout_134 (Dropout)       (None, 512, 512, 16)         0         ['conv2d_148[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_149 (Conv2D)         (None, 512, 512, 1)          17        ['dropout_134[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 980097 (3.74 MB)\n",
      "Trainable params: 980097 (3.74 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Testing Image Input Shape:  (1, 512, 512, 1)\n",
      "Testing Mask Input Shape: (1, 512, 512, 1)\n",
      "1/1 [==============================] - 1s 841ms/step\n"
     ]
    }
   ],
   "source": [
    "# Image Parameters\n",
    "IMAGE_WDITH = 512\n",
    "IMAGE_HEIGHT = 512\n",
    "IMAGE_CHANNELS = 1\n",
    "\n",
    "# Training, Testing and Validation Parameters\n",
    "# TRAINING_VOLUMES = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "# VALIDATION_VOLUMES = [9]\n",
    "\n",
    "# Hyperparameters\n",
    "N_CLASSES = 2\n",
    "N_INPUT_CHANNELS = 1\n",
    "# PATCH_SIZE = (32, 32)\n",
    "# PATCH_STRIDE = (32, 32)\n",
    "\n",
    "# Data Preparation Parameters\n",
    "CONTENT_THRESHOLD = 0.3 # To Get Rid of Useless Information in the Image\n",
    "\n",
    "# Training Parameters\n",
    "N_EPOCHS = 200\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 3\n",
    "MODEL_FNAME_PATTERN = 'model.h5'\n",
    "OPTIMISER = 'Adam'\n",
    "# LOSS = 'categorical_crossentropy'\n",
    "LOSS = 'binary_crossentropy'\n",
    "dropout_rate = 0.40\n",
    "\n",
    "\n",
    "print('-'*30)\n",
    "print('Loading and preprocessing train data...')\n",
    "print('-'*30)\n",
    "\n",
    "images_train, images_val, labels_train, labels_val = train_test_split(imgs_train, imgs_mask_train, test_size=0.2, random_state=0)\n",
    "print('Training Image Input Shape: ', images_train.shape)\n",
    "print('Training Mask Input Shape: ', labels_train.shape)\n",
    "print('Validation Image Input Shape: ', images_val.shape)\n",
    "print('Validation Mask Input Shape: ', labels_val.shape)\n",
    "\n",
    "\n",
    "print('-'*30)\n",
    "print('Creating and compiling model...')\n",
    "print('-'*30)\n",
    "\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=PATIENCE), # early stopping\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=MODEL_FNAME_PATTERN, save_best_only=True) # save the best based on validation\n",
    "]\n",
    "\n",
    "unet = get_unet()\n",
    "unet.compile(optimizer=OPTIMISER, loss=LOSS)\n",
    "unet.fit(\n",
    "    x=images_train, \n",
    "    y=labels_train,\n",
    "    validation_data=(images_val, labels_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=N_EPOCHS,\n",
    "    callbacks=my_callbacks,\n",
    "    verbose=1)\n",
    "\n",
    "unet.summary()\n",
    "\n",
    "unet = get_unet()\n",
    "unet.compile(optimizer=OPTIMISER, loss=LOSS)\n",
    "unet.load_weights('model.h5')\n",
    "\n",
    "testing_scans_processed = images_train[0]\n",
    "testing_masks_processed = labels_train[0]\n",
    "testing_scans_processed = np.reshape(testing_scans_processed, (1, 512, 512, 1))\n",
    "testing_masks_processed = np.reshape(testing_masks_processed, (1, 512, 512, 1))\n",
    "\n",
    "# testing_labels_processed = tf.keras.utils.to_categorical(testing_masks_processed, num_classes=2, dtype='float32')\n",
    "\n",
    "print('Testing Image Input Shape: ',testing_scans_processed.shape)\n",
    "print('Testing Mask Input Shape:',testing_masks_processed.shape)\n",
    "\n",
    "prediction = unet.predict(x=testing_scans_processed)\n",
    "# prediction = np.argmax(prediction, axis=3)\n",
    "# prediction = np.reshape(prediction[0], (512, 512))\n",
    "\n",
    "# plt.imshow(prediction, cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice Similarity Coefficient (DSC) Metric Value:  [4.0989216e-06]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def dice_coefficient(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred, axis=(1, 2, 3))\n",
    "    union = np.sum(y_true, axis=(1, 2, 3)) + np.sum(y_pred, axis=(1, 2, 3))\n",
    "    dice = (2.0 * intersection) / (union + 1e-7)  # Adding a small epsilon to avoid division by zero\n",
    "    return dice\n",
    "\n",
    "def assd(y_true, y_pred, spacing):\n",
    "    surface_distances = surface_distance(y_true, y_pred, spacing)\n",
    "    avg_surface_distance = np.mean(surface_distances)\n",
    "    return avg_surface_distance\n",
    "\n",
    "def surface_distance(y_true, y_pred, spacing):\n",
    "    true_surface = find_surface_points(y_true, spacing)\n",
    "    pred_surface = find_surface_points(y_pred, spacing)\n",
    "    \n",
    "    if true_surface.shape[0] == 0 or pred_surface.shape[0] == 0:\n",
    "        raise ValueError(\"One or both surface point arrays are empty.\")\n",
    "    \n",
    "    try:\n",
    "        surface_distances_true_to_pred = directed_hausdorff(true_surface, pred_surface)[0]\n",
    "        surface_distances_pred_to_true = directed_hausdorff(pred_surface, true_surface)[0]\n",
    "        surface_distances = np.concatenate([surface_distances_true_to_pred, surface_distances_pred_to_true])\n",
    "    except ValueError as e:\n",
    "        print(\"Error occurred during Hausdorff distance calculation:\", e)\n",
    "        raise\n",
    "    \n",
    "    return surface_distances\n",
    "\n",
    "def find_surface_points(mask, spacing):\n",
    "    mask_padded = np.pad(mask, 1, mode='constant')\n",
    "    mask_padded_diff = np.diff(mask_padded.astype(int), axis=0)\n",
    "    \n",
    "    surface_points = []\n",
    "    for z in range(mask_padded_diff.shape[0]):\n",
    "        surface_indices = np.where(mask_padded_diff[z] != 0)\n",
    "        if len(surface_indices[0]) > 0:\n",
    "            surface_points.extend(list(zip(surface_indices[0], surface_indices[1])))\n",
    "    \n",
    "    surface_points = np.array(surface_points)\n",
    "    surface_points_phys = surface_points * spacing\n",
    "    \n",
    "    return surface_points_phys\n",
    "\n",
    "def volume_error(y_true, y_pred):\n",
    "    true_volume = np.sum(y_true)\n",
    "    pred_volume = np.sum(y_pred)\n",
    "    volume_error = np.abs(true_volume - pred_volume) / true_volume\n",
    "    return volume_error\n",
    "\n",
    "\n",
    "DSC = dice_coefficient(testing_masks_processed, prediction)\n",
    "print('Dice Similarity Coefficient (DSC) Metric Value: ', DSC)\n",
    "print('\\n')\n",
    "\n",
    "# VError = volume_error(testing_masks_processed, prediction)\n",
    "# print('Volume Error (VError) Metric Value: ', VError)\n",
    "# print('\\n')\n",
    "\n",
    "# spacing = 1\n",
    "# ASSD = assd(testing_masks_processed, prediction, spacing)\n",
    "# print('Average Symmetric Surface Distance (ASSD) Metric Value: ', ASSD)\n",
    "# print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512, 512, 1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512, 512, 1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_masks_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 512, 512, 1)\n",
      "(1, 512, 512, 1)\n",
      "1/1 [==============================] - 1s 688ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testing_scans_processed = images_train[0]\n",
    "testing_masks_processed = labels_train[0]\n",
    "testing_scans_processed = np.reshape(testing_scans_processed, (1, 512, 512, 1))\n",
    "testing_masks_processed = np.reshape(testing_masks_processed, (1, 512, 512, 1))\n",
    "\n",
    "testing_labels_processed = tf.keras.utils.to_categorical(testing_masks_processed, num_classes=2, dtype='float32')\n",
    "\n",
    "print(testing_scans_processed.shape)\n",
    "print(testing_masks_processed.shape)\n",
    "\n",
    "prediction = unet.predict(x=testing_scans_processed)\n",
    "prediction = np.argmax(prediction, axis=3)\n",
    "\n",
    "prediction = np.reshape(prediction, (512, 512))\n",
    "\n",
    "plt.imshow(prediction, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_labels_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Model\n",
    "# from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, BatchNormalization, Activation, Conv2DTranspose, concatenate\n",
    "# from keras.layers.core import Dropout\n",
    "# import tensorflow as tf\n",
    "# # from tensorflow.keras.models import Model\n",
    "# from keras.layers import *\n",
    "# from keras.optimizers import Adam\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import numpy as np\n",
    "\n",
    "# # Define U-Net model\n",
    "# def unet_model(input_shape):\n",
    "#     inputs = Input(input_shape)\n",
    "    \n",
    "#     # Encoder\n",
    "#     conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "#     conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "#     pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "#     conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "#     conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "#     pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "#     conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "#     conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "#     pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "#     conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
    "#     conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "#     drop4 = Dropout(0.5)(conv4)\n",
    "#     pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "    \n",
    "#     # Bottleneck\n",
    "#     conv5 = Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
    "#     conv5 = Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
    "#     drop5 = Dropout(0.5)(conv5)\n",
    "    \n",
    "#     # Decoder\n",
    "#     up6 = Conv2D(512, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(drop5))\n",
    "#     merge6 = concatenate([drop4, up6], axis=3)\n",
    "#     conv6 = Conv2D(512, 3, activation='relu', padding='same')(merge6)\n",
    "#     conv6 = Conv2D(512, 3, activation='relu', padding='same')(conv6)\n",
    "    \n",
    "#     up7 = Conv2D(256, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv6))\n",
    "#     merge7 = concatenate([conv3, up7], axis=3)\n",
    "#     conv7 = Conv2D(256, 3, activation='relu', padding='same')(merge7)\n",
    "#     conv7 = Conv2D(256, 3, activation='relu', padding='same')(conv7)\n",
    "    \n",
    "#     up8 = Conv2D(128, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv7))\n",
    "#     merge8 = concatenate([conv2, up8], axis=3)\n",
    "#     conv8 = Conv2D(128, 3, activation='relu', padding='same')(merge8)\n",
    "#     conv8 = Conv2D(128, 3, activation='relu', padding='same')(conv8)\n",
    "    \n",
    "#     up9 = Conv2D(64, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv8))\n",
    "#     merge9 = concatenate([conv1, up9], axis=3)\n",
    "#     conv9 = Conv2D(64, 3, activation='relu', padding='same')(merge9)\n",
    "#     conv9 = Conv2D(64, 3, activation='relu', padding='same')(conv9)\n",
    "    \n",
    "#     outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "    \n",
    "#     model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Dice Coefficient Loss Function \n",
    "# def dice_coefficient(y_true, y_pred):\n",
    "#     smooth = 1e-5\n",
    "#     intersection = tf.reduce_sum(y_true * y_pred)\n",
    "#     union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n",
    "#     dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "#     return 1.0 - dice\n",
    "\n",
    "\n",
    "# # Reformat image data structure\n",
    "# training_scans_reshaped = np.concatenate(preprocessed_images, axis=0)\n",
    "# training_scans = training_scans_reshaped.reshape((-1, 512, 512, 1))\n",
    "# train_mask_tibia_labels_reshaped = np.concatenate(preprocessed_masks, axis=0)\n",
    "# train_mask_tibia_labels = train_mask_tibia_labels_reshaped.reshape((-1, 512, 512, 1))\n",
    "\n",
    "# # Split the data into training and validation sets\\\n",
    "# images_train, images_val, labels_train, labels_val = train_test_split(training_scans, train_mask_tibia_labels, test_size=0.2, random_state=0)\n",
    "# unseen_scan_model = np.array(training_scans[2][100])\n",
    "# images_train = images_train.astype('float32') / 255.0\n",
    "# images_val = images_val.astype('float32') / 255.0\n",
    "\n",
    "# print(images_train.shape)\n",
    "# print(labels_train.shape)\n",
    "# print(images_train.dtype)\n",
    "# print(labels_train.dtype)\n",
    "# print(images_val.shape)\n",
    "# print(labels_val.shape)\n",
    "# print(images_val.dtype)\n",
    "# print(labels_val.dtype)\n",
    "# print(unseen_scan_model.shape)\n",
    "\n",
    "# # Expand dimensions for the channel (grayscale) dimension\n",
    "# # images_train = np.expand_dims(images_train, axis=-1)\n",
    "# # images_val = np.expand_dims(images_val, axis=-1)\n",
    "# # labels_train = np.expand_dims(labels_train, axis=-1)\n",
    "# # labels_val = np.expand_dims(labels_val, axis=-1)\n",
    "\n",
    "# # Create an instance of the U-Net model\n",
    "# input_shape = (512, 512, 1)  # For grayscale images\n",
    "\n",
    "# # Create an instance of the U-Net model\n",
    "# model = unet_model(input_shape)\n",
    "\n",
    "# # Compile the model\n",
    "# # Binary Cross Entropy Loss Function\n",
    "# model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Dice Coefficient Loss Function\n",
    "# # model.compile(optimizer=Adam(), loss=dice_coefficient, metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# # Hyperparameter tuning -> batch_size\n",
    "# model.fit(x=images_train, y=labels_train, batch_size=32, epochs=1, validation_data=(images_val, labels_val))\n",
    "# # Evaluate the model\n",
    "# loss, accuracy = model.evaluate(x=images_val, y=labels_val)\n",
    "\n",
    "# # Perform inference on new, unseen MRI scans\n",
    "# predictions = model.predict(unseen_scan_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
