{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part IV Project - H-DenseUNet CNN Implementation for Automated Segmentation of MRI Scans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps:\n",
    "\n",
    "1) Vectorise and allow preprocessing steps to apply to entire individual patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from random import randint\n",
    "import SimpleITK as sitk\n",
    "\n",
    "#CNN Libraries\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, BatchNormalization, Activation, Conv2DTranspose, concatenate\n",
    "from keras.layers.core import Dropout\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.models import Model\n",
    "\n",
    "#Visualisation Libraries\n",
    "import plotly.graph_objects as go\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install opencv-python\n",
    "# %pip install keras\n",
    "# %pip install tensorflow==2.6\n",
    "# %pip install --upgrade tensorflow\n",
    "# %pip install protobuf==3.20.0\n",
    "# %pip uninstall tensorflow\n",
    "# %pip install tensorflow\n",
    "# %pip install protobuf\n",
    "# %pip install SimpleITK\n",
    "# %pip install tensorflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pydicom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15900\\3226750045.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Read in Diacom file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mexample_mri_scan_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydicom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdcmread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"D:/MRI - Tairawhiti/AutoBindWATER_450/IM-0046-0760.dcm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Assessing metadata within Diacom file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pydicom' is not defined"
     ]
    }
   ],
   "source": [
    "# Understanding the data format\n",
    "\n",
    "# Read in Diacom file\n",
    "example_mri_scan_file = pydicom.dcmread(\"D:/MRI - Tairawhiti/AutoBindWATER_450/IM-0046-0760.dcm\")\n",
    "\n",
    "# Assessing metadata within Diacom file\n",
    "example_mri_scan = example_mri_scan_file.pixel_array\n",
    "\n",
    "print('Height and Width of Single MRI Scan: ', example_mri_scan.shape)\n",
    "\n",
    "plt.imshow(example_mri_scan, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_3d_to_2d(array_3d):\n",
    "    # Get the dimensions of the 3D array\n",
    "    depth, height, width = array_3d.shape\n",
    "    \n",
    "    # Reshape the 3D array to a 2D array\n",
    "    array_2d = np.reshape(array_3d, (depth, height * width))\n",
    "    \n",
    "    return array_2d\n",
    "\n",
    "def flatten_2d_array(arr):\n",
    "    flattened = []\n",
    "    for row in arr:\n",
    "        flattened.extend(row)\n",
    "    return flattened"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read In MRI Scan Data Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in entire scan of single patient\n",
    "# folders = [f for f in os.listdir('MRI Scans - Tairawhiti') if os.path.isdir(os.path.join('MRI Scans - Tairawhiti', f))]\n",
    "def ListFolders(directory):\n",
    "    folder_names = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for folder in dirs:\n",
    "            folder_names.append(folder)\n",
    "    return folder_names\n",
    "\n",
    "def read_dicom_files(directory):\n",
    "    dicom_files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        if os.path.isfile(filepath) and filename.endswith('.dcm'):\n",
    "            try:\n",
    "                dicom_file = pydicom.dcmread(filepath)\n",
    "                dicom_files.append(dicom_file)\n",
    "            except pydicom.errors.InvalidDicomError:\n",
    "                print(f\"Skipping file: {filename}. It is not a valid DICOM file.\")\n",
    "    return dicom_files\n",
    "\n",
    "\n",
    "scans_path = 'D:/MRI - Tairawhiti'\n",
    "folders = ListFolders(scans_path)\n",
    "scan_pixel_data = []\n",
    "\n",
    "for paitent in folders:\n",
    "    single_scan_pixel_data = []\n",
    "    single_paitent_scans_path =  scans_path + '/{}'.format(paitent)\n",
    "    dicom_files = read_dicom_files(single_paitent_scans_path)\n",
    "    for i in range (len(dicom_files)):\n",
    "        single_scan_pixel_data.append(dicom_files[i].pixel_array)\n",
    "    scan_pixel_data.append(single_scan_pixel_data)\n",
    "\n",
    "training_scans = flatten_2d_array(scan_pixel_data)\n",
    "training_scans = np.array(training_scans)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotly full body scan\n",
    "def plotly_visualize_mri_scans(scans):\n",
    "    # Create a figure object\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Iterate over the scans\n",
    "    for i, scan in enumerate(scans):\n",
    "        # Create a 3D surface plot for each scan\n",
    "        surface = go.Surface(z=scan, colorscale='gray', showscale=False)\n",
    "        fig.add_trace(surface)\n",
    "\n",
    "    # Set layout properties\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            aspectratio=dict(x=1, y=1, z=0.4),\n",
    "            xaxis=dict(visible=False),\n",
    "            yaxis=dict(visible=False),\n",
    "            zaxis=dict(visible=False),\n",
    "        ),\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    # Create animation frames\n",
    "    frames = [dict(data=[go.Surface(z=scan)]) for scan in scans]\n",
    "    fig.frames = frames\n",
    "\n",
    "    # Set animation properties\n",
    "    animation = dict(\n",
    "        frame=dict(duration=100),\n",
    "        fromcurrent=True,\n",
    "        transition=dict(duration=500, easing='quadratic-in-out'),\n",
    "        mode='immediate'\n",
    "    )\n",
    "\n",
    "    # Add animation controls\n",
    "    fig.update_layout(updatemenus=[dict(type='buttons', showactive=False,\n",
    "                                        buttons=[dict(label='Play',\n",
    "                                                      method='animate',\n",
    "                                                      args=[None, animation])])])\n",
    "\n",
    "    # Show the figure\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visulisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matplotlib grayscale plot within python\n",
    "def IndividualScanVisualisationGrayScalePlot(scan):\n",
    "    plt.imshow(scan, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "# Creates scan visualisation as pop up in new window\n",
    "def IndividualScanVisualisation(scan):\n",
    "    plt.imshow(scan, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocessingMRI(mri_scan_data, Normalization, Resizing, Cropping, Filtering):\n",
    "\n",
    "    # Normalize image intensity (Image Normalization)\n",
    "    if (Normalization == True):\n",
    "        start_time = time.time()\n",
    "        img_norm = mri_scan_data.astype(np.float32) / 255.0\n",
    "        print(\"Normalization time: %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "    # Resize image slices to 256x256 from 512x512 (Image Resizing)\n",
    "    if (Resizing == True):\n",
    "        start_time = time.time()\n",
    "        resized_slices = []\n",
    "        for i in range(img_norm.shape[0]):\n",
    "            resized_slices.append(cv2.resize(img_norm[i], (256, 256)))\n",
    "        img_resized = np.stack(resized_slices)\n",
    "        print(\"Resizing time: %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "    # Crop image slices to remove irrelevant information (Image cropping)\n",
    "    if (Cropping == True):\n",
    "        start_time = time.time()\n",
    "        img_cropped = img_resized[:, 50:200, 50:200]\n",
    "        print(\"Cropping time: %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "    # Apply median filtering to each image slice to remove noise (Image Filtering - Noise Reduction)\n",
    "    if (Filtering == True):\n",
    "        start_time = time.time()\n",
    "        img_filtered = np.zeros_like(img_cropped)\n",
    "        for i in range(img_cropped.shape[0]):\n",
    "            img_filtered[i] = cv2.medianBlur(img_cropped[i], 1)\n",
    "        print(\"Filtering time: %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "    return img_filtered"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImageDataAugmentation(img, num_augmentations):\n",
    "    augmented_images = []\n",
    "\n",
    "    for i in range(num_augmentations):\n",
    "        \n",
    "        #Apply a random rotation to the image\n",
    "        angle = randint(-15, 15)\n",
    "        M = cv2.getRotationMatrix2D((img.shape[1] / 2, img.shape[0] / 2), angle, 1)\n",
    "        rotated_img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
    "\n",
    "        #Apply a random scaling to the image\n",
    "        scale = randint(80, 120) / 100.0\n",
    "        scaled_img = cv2.resize(rotated_img, None, fx=scale, fy=scale)\n",
    "\n",
    "        #Apply a random horizontal flip to the image\n",
    "        if randint(0, 1):\n",
    "            flipped_img = cv2.flip(scaled_img, 1)\n",
    "        else:\n",
    "            flipped_img = scaled_img\n",
    "\n",
    "        #Append the augmented image to the list\n",
    "        augmented_images.append(flipped_img)\n",
    "\n",
    "    #Convert the list of augmented images to a NumPy array\n",
    "    augmented_images = np.array(augmented_images)\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "# def ImageDataAugmentation(img, num_augmentations):\n",
    "#     augmented_images_instances = []\n",
    "#     for i in range (len(img)):\n",
    "#         augmented_images = ImageDataAugmentationFunc(img[i], num_augmentations)\n",
    "#         # list(augmented_images).append(img[i])\n",
    "#         augmented_images_instances.append(augmented_images)\n",
    "\n",
    "#     augmented_images_instances = flatten_3d_to_2d(np.array(augmented_images_instances))\n",
    "\n",
    "#     return augmented_images_instances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Dataset\n",
    "\n",
    "This wont work on entire paitents yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitDataset(pixel_data, train_pct, val_pct, test_pct):\n",
    "    #Calculate the number of slices for each set\n",
    "    num_slices = pixel_data.shape[0]\n",
    "    num_train = int(num_slices * train_pct)\n",
    "    num_val = int(num_slices * val_pct)\n",
    "    num_test = num_slices - num_train - num_val\n",
    "\n",
    "    #Shuffle the indices\n",
    "    indices = np.random.permutation(num_slices)\n",
    "\n",
    "    #Split the indices into sets\n",
    "    train_indices = indices[:num_train]\n",
    "\n",
    "    val_indices = indices[num_train:num_train+num_val]\n",
    "    test_indices = indices[num_train+num_val:]\n",
    "\n",
    "    #Split the data into sets\n",
    "    train_data = pixel_data[train_indices]\n",
    "    val_data = pixel_data[val_indices]\n",
    "    test_data = pixel_data[test_indices]\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block(x, filters, num_layers):\n",
    "    for _ in range(num_layers):\n",
    "        conv = Conv2D(filters=filters, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "        x = concatenate([x, conv], axis=-1)\n",
    "    return x\n",
    "\n",
    "def transition_down(x, filters):\n",
    "    x = Conv2D(filters=filters, kernel_size=(1, 1), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    return x\n",
    "\n",
    "def transition_up(x, filters):\n",
    "    x = Conv2DTranspose(filters=filters, kernel_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    return x\n",
    "\n",
    "def H_DenseUNet(input_shape, num_classes):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    # Initial Convolution Block\n",
    "    conv1 = Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
    "\n",
    "    # Downsample path\n",
    "    down1 = dense_block(conv1, filters=64, num_layers=4)\n",
    "    pool1 = transition_down(down1, filters=128)\n",
    "\n",
    "    down2 = dense_block(pool1, filters=128, num_layers=4)\n",
    "    pool2 = transition_down(down2, filters=256)\n",
    "\n",
    "    down3 = dense_block(pool2, filters=256, num_layers=4)\n",
    "    pool3 = transition_down(down3, filters=512)\n",
    "\n",
    "    down4 = dense_block(pool3, filters=512, num_layers=4)\n",
    "\n",
    "    # Upsample path\n",
    "    up4 = transition_up(down4, filters=256)\n",
    "    up4 = concatenate([up4, down3], axis=-1)\n",
    "    up4 = dense_block(up4, filters=256, num_layers=4)\n",
    "\n",
    "    up3 = transition_up(up4, filters=128)\n",
    "    up3 = concatenate([up3, down2], axis=-1)\n",
    "    up3 = dense_block(up3, filters=128, num_layers=4)\n",
    "\n",
    "    up2 = transition_up(up3, filters=64)\n",
    "    up2 = concatenate([up2, down1], axis=-1)\n",
    "    up2 = dense_block(up2, filters=64, num_layers=4)\n",
    "\n",
    "    # Output\n",
    "    outputs = Conv2D(filters=num_classes, kernel_size=(1, 1), activation='softmax')(up2)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask Creation (Ground Truth for Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred, smooth=1e-7):\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=(1, 2, 3))\n",
    "    union = tf.reduce_sum(y_true, axis=(1, 2, 3)) + tf.reduce_sum(y_pred, axis=(1, 2, 3))\n",
    "    dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "    return (tf.reduce_mean(dice))\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return (1.0 - dice_coefficient(y_true, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'H_DenseUNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8832\\3310056244.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# 7 - Skin: Represents the skin covering the lower limb.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# model = H_DenseUNet(input_shape = (256, 256, 1), num_classes = 7)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mH_DenseUNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_masks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_masks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmri_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseg_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'H_DenseUNet' is not defined"
     ]
    }
   ],
   "source": [
    "# Preprocessing \n",
    "# individual_preprocessed_image_scan = PreprocessingMRI(training_scans, Normalization = True, Resizing = True, Cropping = True, Filtering = True)\n",
    "# augmented_image_scan = ImageDataAugmentation(individual_preprocessed_image_scan[0], num_augmentations = 10)\n",
    "# augmented_image_scan = np.apply_along_axis(ImageDataAugmentation, axis=1, arr=individual_preprocessed_image_scan)\n",
    "# augmented_image_scan = ImageDataAugmentation(individual_preprocessed_image_scan, num_augmentations = 10)\n",
    "# train, validation, test = SplitDataset(augmented_image_scan, train_pct = 0.7, val_pct = 0.2, test_pct = 0.1)\n",
    "\n",
    "\n",
    "# Model Architecture\n",
    "# input_shape: Structure of input data and since we are using grayscale data \n",
    "# num_classes: Number of segmentation classes or categories (potentially requires tuning idk yet, but remains fixed for all layers)\n",
    "# 1 - Background: Represents the background or non-limb regions.\n",
    "# 2 - Femur: Represents the thigh bone.\n",
    "# 3 - Tibia: Represents the shin bone.\n",
    "# 4 - Fibula: Represents the smaller bone in the lower leg.\n",
    "# 5 - Patella: Represents the kneecap.\n",
    "# 6 - Muscles: Represents the muscles of the lower limb.\n",
    "# 7 - Skin: Represents the skin covering the lower limb.\n",
    "# model = H_DenseUNet(input_shape = (256, 256, 1), num_classes = 7)\n",
    "model = H_DenseUNet(input_shape=(256,256,1), n_classes=7)\n",
    "\n",
    "#Train-Validation-Test Split\n",
    "# train_images, val_images, train_masks, val_masks = train_test_split(mri_array, seg_array, test_size=0.2, random_state=42)\n",
    "\n",
    "# Loss Function and Metrics\n",
    "# model.compile(optimizer='adam', loss=dice_loss, metrics=[dice_coefficient])\n",
    "\n",
    "# Define batch size and number of epochs\n",
    "# batch_size = 8\n",
    "# num_epochs = 10\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(train_images, train_masks, validation_data=(val_images, val_masks), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Window Visualisation\n",
    "# IndividualScanVisualisation(augmented_image_scan[0])\n",
    "\n",
    "# Matplotlib Grayscale Plot\n",
    "# IndividualScanVisualisationGrayScalePlot(augmented_image_scan[0])\n",
    "\n",
    "plotly_visualize_mri_scans(training_scans[:1015])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
